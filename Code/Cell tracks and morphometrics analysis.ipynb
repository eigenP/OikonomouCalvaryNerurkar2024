{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Imports**"
      ],
      "metadata": {
        "id": "DPAfhwTDwOmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvhRbHmCXky3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install HDBSCAN and UMAP\n",
        "\n",
        "!pip install uv\n",
        "\n",
        "try:\n",
        "    import hdbscan\n",
        "    import umap\n",
        "except ImportError:\n",
        "    !uv pip install --system hdbscan\n",
        "    !uv pip install --system umap-learn\n",
        "    import hdbscan\n",
        "    import umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JWBjoHMXp71",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "def check_for_nans(df, df_name):\n",
        "    \"\"\"\n",
        "    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): DataFrame to be checked for NaN values.\n",
        "    df_name (str): The name of the DataFrame as a string, used for printing.\n",
        "    \"\"\"\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = df.columns[df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "        for col in nan_columns:\n",
        "            nan_count = df[col].isna().sum()\n",
        "            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n",
        "    else:\n",
        "        print(f\"No NaN values found in {df_name}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load your data and pre-process**"
      ],
      "metadata": {
        "id": "fc9LsKO2xYn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgVQO9YiQmeT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNoLh6rX0Dd3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Load your data folder\n",
        "\n",
        "#@markdown ### Select path to experiment folder\n",
        "Data_folder = r\"/content/gdrive/MyDrive/\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "filenames_ = os.listdir(Data_folder)\n",
        "\n",
        "#@markdown ### Setup rule to select the right files in your folder (currently, match a substring in the filename of each imge, e.g. file extension)\n",
        "import re as regex\n",
        "\n",
        "# Regex pattern to match files that end with .ome.tiff\n",
        "regex_pattern = \".csv\" #@param {type:\"string\"}\n",
        "\n",
        "# Filtering the list using the regex\n",
        "filenames_filtered = sorted([f for f in filenames_ if regex.search(regex_pattern, f)])\n",
        "filenames_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qQo6XJV1wY-"
      },
      "outputs": [],
      "source": [
        "#@title Concatenate btrack files and assign genotype\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "btrack_list_of_dfs = []\n",
        "\n",
        "#@markdown ### Setup rule(s) to assign genotype/condition (must match a substring in the filename of each image) - repeat as much as needed\n",
        "conditon_1 = \"condition_1\" #@param {type:\"string\"}\n",
        "conditon_2 = \"condition_2\" #@param {type:\"string\"}\n",
        "\n",
        "for fname in filenames_filtered:\n",
        "    btrack_csv_to_df = pd.read_csv(os.path.join(Data_folder, fname))\n",
        "    if conditon_1 in fname:\n",
        "      btrack_csv_to_df['Condition'] = conditon_1\n",
        "    elif conditon_2 in fname:\n",
        "      btrack_csv_to_df['Condition'] = conditon_2\n",
        "    with pd.option_context(\"display.max_columns\", None\n",
        "                          ):\n",
        "        with pd.option_context(\"display.max_rows\", 2):\n",
        "            display(btrack_csv_to_df)\n",
        "    print(btrack_csv_to_df.shape)\n",
        "    btrack_list_of_dfs.append(btrack_csv_to_df)\n",
        "\n",
        "btrack_df = pd.concat(btrack_list_of_dfs, ignore_index=True)\n",
        "\n",
        "print(btrack_df.shape)\n",
        "print(btrack_df.columns)\n",
        "\n",
        "with pd.option_context(\"display.max_columns\", None):\n",
        "    with pd.option_context(\"display.max_rows\", 6):\n",
        "        display(btrack_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqu0BbTf-6I8"
      },
      "outputs": [],
      "source": [
        "#@title Add and adjust features / columns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Add sample ID column\n",
        "btrack_df['sample_id_numeric'] = le.fit_transform(btrack_df.sample_id.values)\n",
        "btrack_df['sample_id_numeric'] = (btrack_df['sample_id_numeric'] + 101) * 100\n",
        "# Add unique_ID for each track\n",
        "btrack_df['ID'] = btrack_df['ID'] + 1\n",
        "btrack_df['ID_unique'] = btrack_df['ID'] * btrack_df['sample_id_numeric']\n",
        "\n",
        "# Add shape index\n",
        "btrack_df['shape_index'] = btrack_df['perimeter'] / np.sqrt(btrack_df['area'])\n",
        "\n",
        "# Add area_zero and last_area (mean of first/last three timepoint to mitigate segmentation errors)\n",
        "btrack_df = btrack_df.sort_values(by=['ID_unique', 't'])\n",
        "def compute_area_zero(group):\n",
        "    mean_area = group.head(3)['area'].mean()\n",
        "    group['area_zero'] = mean_area\n",
        "    return group\n",
        "\n",
        "def compute_last_area(group):\n",
        "    mean_area = group.tail(3)['area'].mean()\n",
        "    group['last_area'] = mean_area\n",
        "    return group\n",
        "\n",
        "btrack_df = btrack_df.groupby('ID_unique').apply(compute_area_zero)\n",
        "btrack_df = btrack_df.groupby('ID_unique').apply(compute_last_area)\n",
        "btrack_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Add normalized area\n",
        "btrack_df['area_norm'] = btrack_df['area'] / btrack_df['area_zero']\n",
        "\n",
        "# Add last_to_intial_area\n",
        "btrack_df['last_to_initial_area'] = btrack_df['last_area'] / btrack_df['area_zero']\n",
        "\n",
        "# Print number of tracks / conditions\n",
        "unique_tracks = btrack_df['ID_unique'].nunique()\n",
        "print(f'Unique tracks: {unique_tracks}')\n",
        "tracks_per_condition = btrack_df.groupby('Condition')['ID_unique'].nunique()\n",
        "print(f'Tracks per condition {tracks_per_condition}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Filter out poorly segmentated/tracked tracks based on area variation\n",
        "\n",
        "# Function to check if any row in the group has a more than ±50% change\n",
        "def has_large_area_variation(df):\n",
        "    df = df.sort_values(by='t')\n",
        "    df['area_change'] = df['area'].pct_change().fillna(0)\n",
        "    # Check if any value in 'area_change' exceeds ±50%\n",
        "    return (df['area_change'].abs() > 0.5).any()\n",
        "\n",
        "# Filter out groups where any row has a large change\n",
        "filtered_df = btrack_df.groupby('ID_unique').filter(lambda x: not has_large_area_variation(x))\n",
        "\n",
        "# Drop the temporary 'area_change' column if needed\n",
        "filtered_df = filtered_df.drop(columns='area_change', errors='ignore')\n",
        "\n",
        "# Print the filtered DataFrame\n",
        "print(filtered_df)"
      ],
      "metadata": {
        "id": "_NhY6_XPIFcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chF91Yhqfphn"
      },
      "source": [
        "#**UMAP analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbJ-8kHBQOsE"
      },
      "outputs": [],
      "source": [
        "#@title Create UMAP dataframe\n",
        "\n",
        "# Choose the columns you want to include\n",
        "columns_to_include = ['orientation', 'minor_axis_length', 'major_axis_length',\n",
        "                      'area', 'perimeter', 'solidity', 'strain', 'AR',\n",
        "                      'area_zero', 'AR_zero'\n",
        "                     ]\n",
        "\n",
        "# Select relevant columns from btrack_df\n",
        "selected_df = btrack_df[columns_to_include]\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "btrack_df.sample_id = le.fit_transform(btrack_df.sample_id.values)\n",
        "\n",
        "# Convert non-numeric values to NaN\n",
        "selected_df = selected_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values\n",
        "selected_df = selected_df.dropna()\n",
        "\n",
        "print(\"Done\")\n",
        "\n",
        "# Display the first few rows of the selected DataFrame\n",
        "selected_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Perform UMAP\n",
        "\n",
        "#@markdown useful UMAP documentation to interpret the parameters:  https://umap-learn.readthedocs.io/en/latest/clustering.html\n",
        "import umap\n",
        "import plotly.offline as pyo\n",
        "\n",
        "#@markdown ###UMAP parameters:\n",
        "\n",
        "n_neighbors = 15 # @param {type:\"raw\"}\n",
        "# n_neighbors = len(selected_df) // 10\n",
        "min_dist = 0.01  # @param {type: \"number\"}\n",
        "# min_dist = 0.5  # @param {type: \"number\"}\n",
        "\n",
        "\n",
        "\n",
        "n_dimension = 2\n",
        "\n",
        "\n",
        "# Initialize UMAP object with the specified settings\n",
        "reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_dimension, random_state=42)\n",
        "# Exclude non-numeric columns when fitting UMAP\n",
        "embedding = reducer.fit_transform(selected_df)\n",
        "# Create dynamic column names based on n_components\n",
        "column_names = [f'UMAP_{i}' for i in range(1, n_dimension + 1)]\n",
        "\n",
        "# # Extract the columns_to_include from selected_df\n",
        "# included_data = selected_df[columns_to_include].reset_index(drop=True)\n",
        "\n",
        "# Concatenate the UMAP embedding with ALL the data columns\n",
        "umap_df = pd.concat([pd.DataFrame(embedding, columns=column_names), btrack_df], axis=1)\n",
        "# umap_df = pd.concat([pd.DataFrame(embedding, columns=column_names), included_data], axis=1)\n",
        "#                                                                     ^ or only the included data\n"
      ],
      "metadata": {
        "id": "-IPgGcnjzkVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw5CgRsmHS9s"
      },
      "outputs": [],
      "source": [
        "#@title Display UMAP\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 5 # @param {type: \"number\"}\n",
        "\n",
        "import warnings\n",
        "\n",
        "# # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "# nan_columns = umap_df.columns[umap_df.isna().any()].tolist()\n",
        "\n",
        "# if nan_columns:\n",
        "#   warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n",
        "#   for col in nan_columns:\n",
        "#     umap_df = umap_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n",
        "\n",
        "# Visualize the UMAP projection\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "sns.scatterplot(x=column_names[0], y=column_names[1], data=umap_df, palette='Set2', s=spot_size)\n",
        "plt.title('UMAP Projection of the Dataset')\n",
        "curr_axis = plt.gca()\n",
        "\n",
        "# Turn spines off because UMAP numbers dont mean anything\n",
        "# curr_axis.set_axis_off()\n",
        "plt.setp(curr_axis.spines.values(), visible=False)\n",
        "# remove ticks and labels for the left axis\n",
        "curr_axis.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHgD1EjtVne8"
      },
      "outputs": [],
      "source": [
        "#@title Color by sample ID and conditions -- see how well mixed observations from each experiemnt are\n",
        "\n",
        "import matplotlib as mpl\n",
        "\n",
        "colors_cycler = plt.rcParams['axes.prop_cycle'].by_key()['color']*10\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1,1, figsize=(6,6))\n",
        "scatter_plot = sns.scatterplot(x='UMAP_1', y='UMAP_2', hue='ID_unique', palette='bright', data=umap_df, s=spot_size, legend = False, ax = axes)\n",
        "axes.set_title(f'Color by: Track ID (cell)')\n",
        "scatter_plot.set_xlim(-10,20)\n",
        "curr_axis = plt.gca()\n",
        "\n",
        "# Turn spines off because UMAP numbers dont mean anything\n",
        "# curr_axis.set_axis_off()\n",
        "plt.setp(curr_axis.spines.values(), visible=False)\n",
        "# remove ticks and labels for the left axis\n",
        "curr_axis.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
        "scatter_plot.set_rasterized(True)\n",
        "\n",
        "\n",
        "# Setting labels\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "\n",
        "# Move the axes labels closer to the origin\n",
        "axes.xaxis.set_label_coords(0.07, -0.03)\n",
        "axes.yaxis.set_label_coords(-0.03, 0.07)\n",
        "\n",
        "\n",
        "\n",
        "# with mpl.rc_context({'text.usetex': False,\n",
        "#                 \"svg.fonttype\": 'none'\n",
        "#                 }):\n",
        "#    plt.savefig(\"/content/gdrive/MyDrive/UMAP_memGFP_trackID_color_merge.svg\", format = 'svg')  # Uncomment to save 2D plot as svg\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1,1, figsize=(6,6))\n",
        "sns.scatterplot(x='UMAP_1', y='UMAP_2', hue='Condition', palette=colors_cycler, data=umap_df, s=spot_size, legend = True, ax = axes)\n",
        "axes.set_title(f'Color by: Condition)')\n",
        "#   plt.savefig(\"content/Umap/HDBSCAN_clusters_2D.pdf\")  # Save 2D plot as PDF\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1,1, figsize=(6,6))\n",
        "scatter_plot = sns.scatterplot(x='UMAP_1', y='UMAP_2', hue='sample_id', palette='bright', data=umap_df, s=spot_size, legend = True, ax = axes, alpha = 0.5)\n",
        "# axes.set_title(f'Color by: Sample')\n",
        "sns.move_legend(axes, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "scatter_plot.set_xlim(-10,20)\n",
        "scatter_plot.set_rasterized(True)\n",
        "curr_axis = plt.gca()\n",
        "\n",
        "# Turn spines off because UMAP numbers dont mean anything\n",
        "# curr_axis.set_axis_off()\n",
        "plt.setp(curr_axis.spines.values(), visible=False)\n",
        "# remove ticks and labels for the left axis\n",
        "curr_axis.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
        "scatter_plot.set_rasterized(True)\n",
        "\n",
        "\n",
        "# Setting labels\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "\n",
        "# Move the axes labels closer to the origin\n",
        "axes.xaxis.set_label_coords(0.07, -0.03)\n",
        "axes.yaxis.set_label_coords(-0.03, 0.07)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_closest_factors(N):\n",
        "    # Starting with the square root of N to find the closest factors\n",
        "    for i in range(int(N**0.5), 0, -1):\n",
        "        if N % i == 0:\n",
        "            return i, N // i\n",
        "\n",
        "\n",
        "def find_nicer_factorization(N):\n",
        "    while True:\n",
        "        factors = find_closest_factors(N)\n",
        "\n",
        "        # Check if the factorization is \"nicer\", i.e., both factors are closer to each other than just 1 and N\n",
        "        if np.abs(factors[0] - np.sqrt(N)) <= np.power(N, 0.25)/2:\n",
        "            return N, factors\n",
        "        else:\n",
        "            # Increment N until a nicer factorization is found\n",
        "            N = N + 1\n",
        "\n",
        "\n",
        "\n",
        "number_of_unique_samples = umap_df['sample_id'].nunique()\n",
        "\n",
        "N, factors = find_nicer_factorization(number_of_unique_samples)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(factors[0],factors[1], figsize=(factors[1] * 3,factors[0] * 3), layout = 'constrained')\n",
        "axes = axes.ravel()\n",
        "\n",
        "color_palette = sns.color_palette('bright')\n",
        "for idx_i, sample_id_ in enumerate(umap_df['sample_id'].unique()):\n",
        "    scatter_plot = sns.scatterplot(x='UMAP_1', y='UMAP_2', color= color_palette[idx_i], alpha=0.5,\n",
        "                    data=umap_df[umap_df['sample_id'] == sample_id_], s=spot_size, ax=axes[idx_i])\n",
        "    # axes[idx_i].set_title(f'Sample: {sample_id_}')\n",
        "    scatter_plot.set_xlim(-10,20)\n",
        "    scatter_plot.set_rasterized(True)\n",
        "\n",
        "    # Turn spines off because UMAP numbers dont mean anything\n",
        "    # curr_axis.set_axis_off()\n",
        "    plt.setp(scatter_plot.spines.values(), visible=False)\n",
        "    # remove ticks and labels for the left axis\n",
        "    scatter_plot.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
        "    # with mpl.rc_context({'text.usetex': False,\n",
        "    #                 \"svg.fonttype\": 'none'\n",
        "    #                 }):\n",
        "    #    plt.savefig(\"/content/gdrive/MyDrive/UMAP_memGFP_sample_color.svg\", format = 'svg')  # Uncomment to save 2D plot as PDF\n",
        "\n",
        "        # Setting labels\n",
        "    plt.xlabel('UMAP 1')\n",
        "    plt.ylabel('UMAP 2')\n",
        "\n",
        "    # Move the axes labels closer to the origin\n",
        "    scatter_plot.xaxis.set_label_coords(0.07, -0.03)\n",
        "    scatter_plot.yaxis.set_label_coords(-0.03, 0.07)\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTONfFfViOzP"
      },
      "outputs": [],
      "source": [
        "#@title Identify clusters using HDBSCAN\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "\n",
        "def mpl_to_plotly(cmap, pl_entries=11, rdigits=2):\n",
        "    # cmap - colormap\n",
        "    cmap = mpl.colormaps[cmap]\n",
        "    # pl_entries - int = number of Plotly colorscale entries\n",
        "    # rdigits - int -=number of digits for rounding scale values\n",
        "    scale = np.linspace(0, 1, pl_entries)\n",
        "    colors = (cmap(scale)[:, :3]*255).astype(np.uint8)\n",
        "    pl_colorscale = [[round(s, rdigits), f'rgb{tuple(color)}'] for s, color in zip(scale, colors)]\n",
        "    return pl_colorscale\n",
        "\n",
        "#@markdown ###HDBSCAN parameters:\n",
        "clustering_data_source = 'umap'  # @param ['umap', 'raw']\n",
        "min_samples = 30 # @param [\"None\"] {type:\"raw\", allow-input: true}\n",
        "min_cluster_size = umap_df.shape[0] // 50 # @param {type:\"raw\"}\n",
        "# min_cluster_size = umap_df.shape[0] // 50\n",
        "metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n",
        "\n",
        "'''\n",
        "min_cluster_size : int, optional (default=5)\n",
        "    The minimum size of clusters; single linkage splits that contain\n",
        "fewer points than this will be considered points \"falling out\" of a\n",
        "cluster rather than a cluster splitting into two new clusters.\n",
        "\n",
        "min_samples : int, optional (default=None)\n",
        "    The number of samples in a neighbourhood for a point to be\n",
        "considered a core point.\n",
        "'''\n",
        "\n",
        "# Apply HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)  # You may need to tune these parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ka7bc1UeuJY"
      },
      "outputs": [],
      "source": [
        "#@title ### Plot the clusters\n",
        "\n",
        "from matplotlib import patheffects\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 5 # @param {type: \"number\"}\n",
        "cmap ='tab20' # @param {type: \"string\"}\n",
        "legend_loc = 'on data' #@param {type: \"string\"}\n",
        "\n",
        "colors = [\"#2f4f4f\", \"#a0522d\", \"#6495ed\", \"#228b22\", \"#ff69b4\",  \"#4b0082\", \"#ff0000\", \"#ffe119\", \"#469990\", \"#00ffff\", \"#0000ff\", \"#ff00ff\", \"#eee8aa\"]\n",
        "\n",
        "\n",
        "if clustering_data_source == 'umap':\n",
        "    clusterer.fit(umap_df[['UMAP_1', 'UMAP_2']])  # Use two UMAP dims for clustering\n",
        "else:\n",
        "    clusterer.fit(selected_df.select_dtypes(include=['number']))\n",
        "\n",
        "# Add the cluster labels to your UMAP DataFrame\n",
        "umap_df['Cluster'] = clusterer.labels_\n",
        "\n",
        "# If the Cluster column already exists in btrack_df, drop it to avoid duplications\n",
        "if 'Cluster' in btrack_df.columns:\n",
        "    btrack_df.drop(columns='Cluster', inplace=True)\n",
        "\n",
        "# Merge the Cluster column from umap_df to btrack_df based on Unique_ID btrack_df = pd.merge btrack_df, umap_df[['Unique_ID', 'Cluster']], on='Unique_ID', how='left')\n",
        "\n",
        "# Handle cases where some rows in btrack_df might not have a corresponding cluster label btrack_df['Cluster'].fillna(-1, inplace=True)  # Assigning -1 to cells that were not assigned to any cluster\n",
        "\n",
        "# Save the DataFrame with the identified clusters btrack_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(4,4))\n",
        "scatter_plot = sns.scatterplot(x='UMAP_1', y='UMAP_2', hue='Cluster', palette=colors, data=umap_df, s=spot_size)\n",
        "plt.title('Clusters Identified by HDBSCAN')\n",
        "current_ax = plt.gca()\n",
        "\n",
        "if legend_loc == 'on data':\n",
        "#### === THIS IS FRON SCANPY === ###\n",
        "# identify centroids to put labels\n",
        "\n",
        "    all_pos = (\n",
        "        pd.DataFrame(umap_df, columns=[\"UMAP_1\", \"UMAP_2\", \"Cluster\"])\n",
        "        .groupby('Cluster', observed=True)\n",
        "        .median()\n",
        "        # Have to sort_index since if observed=True and categorical is unordered\n",
        "        # the order of values in .index is undefined. Related issue:\n",
        "        # https://github.com/pandas-dev/pandas/issues/25167\n",
        "        .sort_index()\n",
        "    )\n",
        "\n",
        "    for label, x_pos, y_pos in all_pos.itertuples():\n",
        "        current_ax.text(\n",
        "            x_pos,\n",
        "            y_pos,\n",
        "            label,\n",
        "            # weight=legend_fontweight,\n",
        "            verticalalignment=\"center\",\n",
        "            horizontalalignment=\"center\",\n",
        "            # fontsize=legend_fontsize,\n",
        "            path_effects= [patheffects.withStroke(linewidth=2, foreground=\"w\")],\n",
        "        )\n",
        "    current_ax.get_legend().remove()\n",
        "else:\n",
        "    sns.move_legend(current_ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "\n",
        "# Turn spines off because UMAP numbers dont mean anything\n",
        "# curr_axis.set_axis_off()\n",
        "plt.setp(current_ax.spines.values(), visible=False)\n",
        "# remove ticks and labels for the left axis\n",
        "current_ax.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
        "\n",
        "scatter_plot.set_rasterized(True)\n",
        "\n",
        "# Setting labels\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "\n",
        "# Move the axes labels closer to the origin\n",
        "scatter_plot.xaxis.set_label_coords(0.09, -0.03)\n",
        "scatter_plot.yaxis.set_label_coords(-0.03, 0.11)\n",
        "\n",
        "# with mpl.rc_context({'text.usetex': False,\n",
        "#                 \"svg.fonttype\": 'none'\n",
        "#                 }):\n",
        "\n",
        "#    plt.savefig(\"/content/UMAP_clusters.svg\", format = 'svg', dpi = 300)  #\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v1d3MbcPlI-"
      },
      "outputs": [],
      "source": [
        "#@title Plot percentages of conditions in each clusters\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group the data by Cluster and Condition and calculate the count of each combination\n",
        "grouped = umap_df.groupby(['Cluster', 'Condition']).size().reset_index(name='Count')\n",
        "\n",
        "# Pivot the data to have Cluster as columns and Condition as rows\n",
        "pivot_df = grouped.pivot(index='Cluster', columns='Condition', values='Count')\n",
        "\n",
        "# Calculate the sum of counts within each Cluster\n",
        "cluster_sum = pivot_df.sum(axis=1)\n",
        "\n",
        "# Calculate the percentages within each Cluster\n",
        "percentage_df = pivot_df.divide(cluster_sum, axis=0) * 100\n",
        "\n",
        "# Plot the bar chart\n",
        "x = percentage_df.index\n",
        "width = 0.5\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.bar(x, percentage_df['memGFP'], width, label='memGFP', color='black')\n",
        "ax.bar(x, percentage_df['myrRFP'], width, bottom=percentage_df['memGFP'], label='myrRFP', color = 'magenta', alpha=0.65)\n",
        "ax.bar(x, percentage_df['deAct'], width, bottom=percentage_df['memGFP'] + percentage_df['myrRFP'], label='deAct', color='green')\n",
        "\n",
        "ax.set_xlabel('Cluster')\n",
        "ax.set_ylabel('Percentage')\n",
        "ax.set_title('Percentage of Conditions for Each Cluster')\n",
        "ax.set_xticks(x)\n",
        "\n",
        "# Move the legend to the right\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwJldu8TU0ll",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ### Plot various features on UMAP space\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "features_list_to_plot = ['strain', 'area_norm', 'AR', 'ML_zero']\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(22, 4), constrained_layout= False)\n",
        "plt.subplots_adjust(wspace=0.1)\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx_i, feature_color_by_ in enumerate(features_list_to_plot):\n",
        "    cutoff_min = np.percentile(umap_df[feature_color_by_], 2.5)\n",
        "    cutoff_max = np.percentile(umap_df[feature_color_by_], 97.5)\n",
        "\n",
        "    scatter_plot = sns.scatterplot(x='UMAP_1', y='UMAP_2', hue=umap_df[feature_color_by_], palette='viridis', data=umap_df,\n",
        "                                    s=spot_size, ax=axes[idx_i], alpha=0.8, legend = False)\n",
        "\n",
        "    scatter_plot.set_rasterized(True)\n",
        "\n",
        "    # Refine colormap based on percentiles\n",
        "    norm = plt.Normalize(cutoff_min, cutoff_max)\n",
        "    scatter_plot.collections[0].set_norm(norm)\n",
        "    scatter_plot.collections[0].set_array(umap_df[feature_color_by_])\n",
        "\n",
        "    plt.setp(scatter_plot.spines.values(), visible=False)\n",
        "    scatter_plot.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
        "    plt.xlabel('UMAP 1')\n",
        "    plt.ylabel('UMAP 2')\n",
        "\n",
        "    if idx_i == 2:\n",
        "        plt.colorbar(scatter_plot.collections[0], ax=axes[idx_i], aspect=40, ticks=([1.5, 4.5]))\n",
        "    elif idx_i == 0:\n",
        "        plt.colorbar(scatter_plot.collections[0], ax=axes[idx_i], aspect=40, ticks=([0.1, 0.6]))\n",
        "    elif idx_i == 3:\n",
        "        colorbar = plt.colorbar(scatter_plot.collections[0], ax=axes[idx_i], aspect=40, ticks=([55.1, 550.7]))\n",
        "        colorbar.set_ticklabels(['25','250'])\n",
        "\n",
        "    elif idx_i == 1:\n",
        "        plt.colorbar(scatter_plot.collections[0], ax=axes[idx_i], aspect=40, ticks=([0.9, 1.4]))\n",
        "\n",
        "\n",
        "\n",
        "    # with mpl.rc_context({'text.usetex': False,\n",
        "    #             \"svg.fonttype\": 'none'\n",
        "    #             }):\n",
        "    #     plt.savefig(\"/content/gdrive/MyDrive/UMAP_strain,area_norm,AR,ML_0.svg\", format = 'svg')  # Uncomment to save 2D plot as svg\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EipvIIX_pHeb"
      },
      "outputs": [],
      "source": [
        "#@title Plot initials features for each cluster\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# Define the features for which you want to create boxplots\n",
        "features = ['area_zero', 'AR_zero', 'ML_zero']\n",
        "\n",
        "# Create a colormap\n",
        "colors = [\"#2f4f4f\", \"#a0522d\", \"#6495ed\", \"#228b22\", \"#ff69b4\",  \"#4b0082\", \"#ff0000\", \"#ffe119\", \"#469990\", \"#00ffff\", \"#0000ff\", \"#ff00ff\", \"#eee8aa\"]\n",
        "\n",
        "num_clusters = len(umap_df['Cluster'].unique())\n",
        "\n",
        "# Create a subplots grid for the boxplots\n",
        "fig, axes = plt.subplots(nrows=len(features), ncols=1, figsize=(7, 16))\n",
        "\n",
        "# Iterate over each feature and create a boxplot in a subplot\n",
        "for i, feature in enumerate(features):\n",
        "\n",
        "    # Extract the first occurrence for each cell\n",
        "    first_occurrence = umap_df.groupby('ID_unique')[feature].first().reset_index()\n",
        "\n",
        "    # Merge with umap_df to include the \"Cluster\" column\n",
        "    first_occurrence = first_occurrence.merge(umap_df[['ID_unique', 'Cluster']], on='ID_unique')\n",
        "\n",
        "    first_occurrence = first_occurrence.drop_duplicates()\n",
        "\n",
        "    # Create a combined plot with boxplot and stripplot\n",
        "    boxplot = sns.boxplot(data=first_occurrence, x='Cluster', y=feature, hue='Cluster', showfliers=False, palette=colors, ax=axes[i], legend = False)\n",
        "    sns.stripplot(data=first_occurrence, x='Cluster', y=feature, hue='Cluster', jitter=True, alpha=0.2, palette=colors, edgecolor='black', linewidth=1, ax=axes[i], legend = False)\n",
        "\n",
        "    boxplot.set_title(f'Boxplot of {feature} by Cluster')\n",
        "    boxplot.set_xlabel('Cluster')\n",
        "    boxplot.set_ylabel(feature)\n",
        "\n",
        "\n",
        "    # Pixel to micron correction\n",
        "    yticklabels_ = np.arange(200, 701, 100, )\n",
        "    axes[0].set_yticks([i_ * (2.205**2) for i_ in yticklabels_])\n",
        "    axes[0].set_yticklabels([str(label) for label in yticklabels_])\n",
        "\n",
        "    yticklabels_ = np.arange(50, 351, 50, )\n",
        "    axes[1].set_yticks([i_ * 2.205 for i_ in yticklabels_])\n",
        "    axes[1].set_yticklabels([str(label) for label in yticklabels_])\n",
        "\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure\n",
        "# with mpl.rc_context({'text.usetex': False, \"svg.fonttype\": 'none'}):\n",
        "#     plt.savefig(\"/content/gdrive/MyDrive/Clusters_initial_features_boxplots.svg\", format='svg') # Uncomment to save 2D plots as svg\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgRHj361jCKK"
      },
      "outputs": [],
      "source": [
        "#@title Plot various features vs strain for each cluster\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.interpolate import CubicSpline\n",
        "\n",
        "METRIC_COLUMN_NAME = ['area_norm', 'AR', 'orientation']\n",
        "\n",
        "# Group by 'ID_unique' and filter\n",
        "\n",
        "def cubic_interpolation_no_duplicates(group, num_points=200):\n",
        "    group = group.sort_values(by='strain')  # Sort by strain\n",
        "    group = group.drop_duplicates(subset='strain')  # Remove duplicates\n",
        "    x = group['strain']\n",
        "    y = group[metric]\n",
        "\n",
        "    f = interp1d(x, y,\n",
        "                 kind='linear',   # Use linear interpolation, vs 'cubic'\n",
        "                 fill_value=('nan'),\n",
        "                 bounds_error=False,\n",
        "                 assume_sorted=True)\n",
        "\n",
        "    new_strains = np.linspace(0, MIN_FINAL_STRAIN, num_points)\n",
        "    new_area_norms = f(new_strains)\n",
        "    return pd.Series(new_area_norms)\n",
        "\n",
        "MIN_FINAL_STRAIN = 0.7\n",
        "\n",
        "# Get the number of unique clusters\n",
        "num_clusters = len(umap_df['Cluster'].unique())\n",
        "\n",
        "# Create a colormap with 'num_clusters' colors\n",
        "colors = [\"#2f4f4f\", \"#a0522d\", \"#6495ed\", \"#228b22\", \"#ff69b4\",  \"#4b0082\", \"#ff0000\", \"#ffe119\", \"#469990\", \"#00ffff\", \"#0000ff\", \"#ff00ff\", \"#eee8aa\"]\n",
        "\n",
        "rolling_window = 10\n",
        "# Create a separate figure\n",
        "fig, axs = plt.subplots(len(METRIC_COLUMN_NAME), num_clusters, figsize=((3 * num_clusters), (3 * len(METRIC_COLUMN_NAME))))\n",
        "\n",
        "# Iterate over each cluster\n",
        "unique_clusters = np.sort(umap_df['Cluster'].unique())\n",
        "\n",
        "for j, cluster in enumerate(unique_clusters):\n",
        "    # Iterate over each metric column\n",
        "    for i, metric in enumerate(METRIC_COLUMN_NAME):\n",
        "        # Get the corresponding subplot\n",
        "        ax = axs[i, j]\n",
        "\n",
        "        cluster_df = umap_df[umap_df['Cluster'] == cluster]\n",
        "        cluster_df = cluster_df[['ID_unique', 'strain', metric]]\n",
        "        cluster_df = cluster_df.groupby('ID_unique').filter(lambda x: x['strain'].max() > MIN_FINAL_STRAIN)\n",
        "\n",
        "        # Filter out groups with less than 3 values for strain\n",
        "        group_counts = cluster_df.groupby('ID_unique').size()\n",
        "        valid_groups = group_counts[group_counts > 3].index\n",
        "        cluster_df = cluster_df[cluster_df['ID_unique'].isin(valid_groups)]\n",
        "\n",
        "        interpolated_data = cluster_df.groupby('ID_unique').apply(cubic_interpolation_no_duplicates)\n",
        "\n",
        "        final_df = interpolated_data\n",
        "        final_df.columns.name = None\n",
        "        final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        mean_values = final_df.mean(axis=0)\n",
        "        std_values = final_df.std(axis=0)\n",
        "\n",
        "        # Apply rolling average to smooth the curves\n",
        "        mean_values_smoothed = mean_values.rolling(rolling_window, center=True).mean()\n",
        "        std_values_smoothed = std_values.rolling(rolling_window, center=True).mean()\n",
        "\n",
        "\n",
        "        strain_values = np.linspace(0, MIN_FINAL_STRAIN, final_df.shape[-1])\n",
        "\n",
        "        color = colors[j]\n",
        "        ax.plot(strain_values, mean_values_smoothed, color=color, label=f'Cluster {cluster}')\n",
        "        ax.fill_between(strain_values, mean_values_smoothed - std_values_smoothed, mean_values_smoothed + std_values_smoothed,\n",
        "                        color=color, alpha=0.3)\n",
        "\n",
        "        # Set the labels and title for the subplot\n",
        "        ax.set_xlabel('Strain')\n",
        "        ax.set_ylabel(f'Mean {metric}')\n",
        "        ax.set_title(f'Cluster {cluster}: Mean {metric}')\n",
        "        ax.set_xlim(0,0.7)\n",
        "        if i == 0:  # First subplot\n",
        "            ax.set_ylim(0.9, 1.5)\n",
        "        elif i == 1:  # Second subplot\n",
        "            ax.set_ylim(1, 5)\n",
        "        elif i == 2:  # Third subplot\n",
        "            ax.set_ylim(-1.5, 1.5)\n",
        "\n",
        "\n",
        "        # Add a legend to the subplot\n",
        "        ax.legend()\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# with mpl.rc_context({'text.usetex': False,\n",
        "#                 \"svg.fonttype\": 'none'\n",
        "#                 }):\n",
        "#     plt.savefig(\"/content/gdrive/MyDrive/Cluster_featuresVSstrain.svg\", format = 'svg') # Uncomment to save 2D plot as svg\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BiXDdKG9uS1"
      },
      "outputs": [],
      "source": [
        "#@title Plot intials features of specific clusters\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.api.types import CategoricalDtype\n",
        "\n",
        "# Define the features for which you want to create boxplots\n",
        "features = ['area_zero', 'AR_zero', 'ML_zero']\n",
        "\n",
        "# Define the clusters you want to plot\n",
        "clusters_to_plot = [1, 3]\n",
        "\n",
        "colors = [\"#2f4f4f\", \"#a0522d\", \"#6495ed\", \"#228b22\", \"#ff69b4\",  \"#4b0082\", \"#ff0000\", \"#ffe119\", \"#469990\", \"#00ffff\", \"#0000ff\", \"#ff00ff\", \"#eee8aa\"]\n",
        "\n",
        "# Create a colormap with 'num_clusters' colors\n",
        "color_map = [colors[cluster + 1] for cluster in clusters_to_plot]\n",
        "\n",
        "# Custom order for the clusters\n",
        "custom_order = [1, 3]\n",
        "\n",
        "# Create a single plot with subplots for each feature\n",
        "fig, axes = plt.subplots(len(features), figsize=(5, 5 * len(features)))\n",
        "\n",
        "# Iterate over each feature and create a subplot\n",
        "for i, feature in enumerate(features):\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Extract the first occurrence for each cell\n",
        "    first_occurrence = umap_df.groupby('ID_unique')[feature].first().reset_index()\n",
        "\n",
        "    # Merge with umap_df to include the \"Cluster\" column\n",
        "    first_occurrence = first_occurrence.merge(umap_df[['ID_unique', 'Cluster']], on='ID_unique')\n",
        "\n",
        "    first_occurrence = first_occurrence.drop_duplicates()\n",
        "\n",
        "    # Filter the data for only the specified clusters\n",
        "    filtered_data = first_occurrence[first_occurrence['Cluster'].isin(clusters_to_plot)].copy()\n",
        "\n",
        "    # Convert 'Cluster' column to categorical variable with custom order\n",
        "    cluster_order = CategoricalDtype(categories=custom_order, ordered=True)\n",
        "    filtered_data.loc[:, 'Cluster'] = filtered_data['Cluster'].astype(cluster_order)\n",
        "\n",
        "    # Create a combined plot with boxplot and stripplot\n",
        "    sns.boxplot(data=filtered_data, x='Cluster', y=feature, hue='Cluster', showfliers=False, palette=color_map, ax=ax)\n",
        "    sns.stripplot(data=filtered_data, x='Cluster', y=feature, hue='Cluster', jitter=True, alpha=0.2, palette=color_map, edgecolor='black', linewidth=1, ax=ax, legend=False)\n",
        "\n",
        "    ax.set_title(f'Boxplot of {feature} for Clusters {clusters_to_plot}')\n",
        "    ax.set_xlabel('Cluster')\n",
        "    ax.set_ylabel(feature)\n",
        "\n",
        "with mpl.rc_context({'text.usetex': False,\n",
        "                \"svg.fonttype\": 'none'\n",
        "                }):\n",
        "    plt.savefig(\"/content/gdrive/MyDrive/Specific_clusters_initial_features.svg\", format = 'svg')  # Save 2D plot as PDF\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3_ZjAuCypID"
      },
      "outputs": [],
      "source": [
        "#@title Plot various features vs strain for specific clusters\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib as mpl\n",
        "\n",
        "\n",
        "METRIC_COLUMN_NAME = ['area_norm', 'AR']\n",
        "\n",
        "# Group by 'ID_unique' and filter\n",
        "def cubic_interpolation_no_duplicates(group, num_points=200):\n",
        "    group = group.sort_values(by='strain')  # Sort by strain\n",
        "    group = group.drop_duplicates(subset='strain')  # Remove duplicates\n",
        "    f = interp1d(group['strain'], group[metric],\n",
        "                 kind='linear',  # cubic\n",
        "                 bounds_error=False, fill_value=\"nan\")\n",
        "    new_strains = np.linspace(0, MIN_FINAL_STRAIN, num_points)\n",
        "    new_area_norms = f(new_strains)\n",
        "    return pd.Series(new_area_norms)\n",
        "\n",
        "MIN_FINAL_STRAIN = 0.7\n",
        "\n",
        "\n",
        "# Get the number of unique clusters\n",
        "num_clusters = len(umap_df['Cluster'].unique())\n",
        "\n",
        "# Create a separate figure\n",
        "fig, ax = plt.subplots(len(METRIC_COLUMN_NAME), 1, figsize=(3, 6), layout = 'constrained')\n",
        "\n",
        "# List of desired clusters\n",
        "desired_clusters = [[1], [3], [1,2,3,4,5,6,7,8,9,10,11]]\n",
        "MAX_FINAL_STRAIN = umap_df.loc[umap_df['Cluster'].isin(desired_clusters), 'strain'].max()\n",
        "\n",
        "\n",
        "colors = [\"#2f4f4f\", \"#a0522d\", \"#6495ed\", \"#228b22\", \"#ff69b4\",  \"#4b0082\", \"#ff0000\", \"#ffe119\", \"#469990\", \"#00ffff\", \"#0000ff\", \"#ff00ff\", \"#eee8aa\"]\n",
        "color_map = [colors[cluster[-1] - 1] for cluster in desired_clusters]\n",
        "\n",
        "# Rolling average window size\n",
        "rolling_window = 10\n",
        "\n",
        "# Iterate over each metric column\n",
        "for i, metric in enumerate(METRIC_COLUMN_NAME):\n",
        "    for idx_i, cluster in enumerate(desired_clusters):\n",
        "        cluster_df = umap_df[umap_df['Cluster'].isin(cluster)]\n",
        "        cluster_df = cluster_df[['ID_unique', 'strain', metric]]\n",
        "        cluster_df = cluster_df.groupby('ID_unique').filter(lambda x: x['strain'].max() > MIN_FINAL_STRAIN)\n",
        "\n",
        "        # Filter out groups with less than 3 values for strain\n",
        "        group_counts = cluster_df.groupby('ID_unique').size()\n",
        "        valid_groups = group_counts[group_counts > 3].index\n",
        "        cluster_df = cluster_df[cluster_df['ID_unique'].isin(valid_groups)]\n",
        "\n",
        "        interpolated_data = cluster_df.groupby('ID_unique').apply(cubic_interpolation_no_duplicates)\n",
        "\n",
        "        final_df = interpolated_data\n",
        "        final_df.columns.name = None\n",
        "        final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        mean_values = final_df.mean(axis=0)\n",
        "        std_values = final_df.std(axis=0)\n",
        "\n",
        "        # Apply rolling average to smooth the curves\n",
        "        mean_values_smoothed = mean_values.rolling(rolling_window, center=True).mean()\n",
        "        std_values_smoothed = std_values.rolling(rolling_window, center=True).mean()\n",
        "\n",
        "        strain_values = np.linspace(0, MIN_FINAL_STRAIN, final_df.shape[-1])\n",
        "\n",
        "        hatch_ = ''\n",
        "        if idx_i == 3:\n",
        "            hatch_ = '/'\n",
        "\n",
        "        color = colors[cluster[-1]+1]\n",
        "        ax[i].plot(strain_values, mean_values_smoothed, color=color, label=f'Cluster {cluster[-1]}')\n",
        "        ax[i].fill_between(strain_values, mean_values_smoothed - std_values_smoothed, mean_values_smoothed + std_values_smoothed,\n",
        "                        color=color, alpha=0.4, hatch = hatch_)\n",
        "\n",
        "    # Set the labels and title for the subplot\n",
        "    ax[i].set_xlabel('Strain')\n",
        "    ax[i].set_ylabel(f'Mean {metric}')\n",
        "    # ax[i].set_title(f'Mean {metric} by cluster')\n",
        "    ax[i].set_xlim(-0.01,0.7)\n",
        "\n",
        "    # Add a legend to the subplot\n",
        "    ax[i].legend()\n",
        "\n",
        "\n",
        "with mpl.rc_context({'text.usetex': False,\n",
        "                    \"svg.fonttype\": 'none'\n",
        "                    }):\n",
        "    plt.savefig(\"/content/Specific_clusters_featuresVSstrain.svg\", format='svg')  # Save 2D plot as SVG\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D3bIjVQRh_0"
      },
      "outputs": [],
      "source": [
        "#@title Plot max normalized area VS initial area\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'umap_df' is the pandas DataFrame containing the data\n",
        "\n",
        "colors = [\"#2f4f4f\", \"#a0522d\", \"#6495ed\", \"#228b22\", \"#ff69b4\",  \"#4b0082\", \"#ff0000\", \"#ffe119\", \"#469990\", \"#00ffff\", \"#0000ff\", \"#ff00ff\", \"#eee8aa\"]\n",
        "\n",
        "# Sort the DataFrame by 'Cluster' column\n",
        "umap_df.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "area_norm_max = []\n",
        "area_zero = []\n",
        "\n",
        "# Group the DataFrame by 'ID_unique' column and calculate the maximum 'area_norm' within each group\n",
        "grouped_df = umap_df.groupby('ID_unique')['area_norm'].max()\n",
        "\n",
        "# Track the clusters for which the legend labels have been added\n",
        "added_clusters = set()\n",
        "\n",
        "# Iterate over unique clusters\n",
        "for ID_unique in umap_df['ID_unique'].unique():\n",
        "    # Append the maximum 'area_norm' value for the current cluster\n",
        "    area_norm_max.append(grouped_df.loc[ID_unique])\n",
        "\n",
        "    # Retrieve the first 'area_zero' value for the current cluster\n",
        "    ID_unique_area_zero = umap_df.loc[umap_df['ID_unique'] == ID_unique, 'area_zero'].iloc[0]\n",
        "    area_zero.append(ID_unique_area_zero)\n",
        "\n",
        "    # Assign color to the current cluster\n",
        "    cluster = umap_df.loc[umap_df['ID_unique'] == ID_unique, 'Cluster'].iloc[0]\n",
        "    color_idx = cluster + 1  # Adjust index to account for clusters starting from -1\n",
        "    color = colors[color_idx]\n",
        "\n",
        "    # Only add the legend label for the cluster if it has not been added before\n",
        "    if cluster not in added_clusters:\n",
        "        plt.scatter(ID_unique_area_zero, grouped_df.loc[ID_unique], s=5, color=color, label=f\"Cluster {cluster}\")\n",
        "        added_clusters.add(cluster)\n",
        "    else:\n",
        "        plt.scatter(ID_unique_area_zero, grouped_df.loc[ID_unique], s=5, color=color, alpha = 0.75)\n",
        "\n",
        "# Set labels for the x-axis and y-axis\n",
        "plt.xlabel('area_zero')\n",
        "plt.ylabel('max_area_norm')\n",
        "\n",
        "# plt.xticks([970.3, 1455.5, 1940.7, 2425.8, 2911, 3396], ['200', '300', '400', '500', '600', '700'])\n",
        "\n",
        "# Set a title for the plot\n",
        "plt.title('max_area_norm vs area_zero')\n",
        "\n",
        "# Add legend\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "# Save the figure\n",
        "with mpl.rc_context({'text.usetex': False, \"svg.fonttype\": 'none'}):\n",
        "    plt.savefig(\"/content/gdrive/MyDrive/max_area_norm VS area_zero.svg\", format='svg')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Plot average conditions per conditions**"
      ],
      "metadata": {
        "id": "e5IekzzM9Ko0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7m0YX1NAMwa"
      },
      "outputs": [],
      "source": [
        "#@title Plot intials features per condition\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the features for which you want to create boxplots\n",
        "features = ['area_zero', 'AR_zero', 'ML_zero']\n",
        "\n",
        "# Create a colormap with 'num_clusters' colors\n",
        "color_map = ['lightgrey', 'darkgreen', 'dimgrey', 'lightgreen']\n",
        "condition_order = ['memGFP', 'myrRFP (from memGFP)', 'deActGS1', \"myrRFP (from deActGS1)\"]\n",
        "palette = dict(zip(condition_order, color_map))  # Map conditions to colors\n",
        "\n",
        "# Define hatch patterns for each condition\n",
        "hatch_dict = {\n",
        "    'memGFP': '//',\n",
        "    'myrRFP (from memGFP)': '',\n",
        "    'deActGS1': '',\n",
        "    'myrRFP (from deActGS1)': '//'\n",
        "}\n",
        "\n",
        "# Create a new figure and subplots with the desired layout\n",
        "fig, axes = plt.subplots(3, 1, figsize=(7, 20))\n",
        "\n",
        "# Create a separate boxplot for each feature\n",
        "for i, feature in enumerate(features):\n",
        "    # Extract the first occurrence for each cell\n",
        "    first_occurrence = filtered_subset_df.groupby('ID_unique')[feature].first().reset_index()\n",
        "\n",
        "    # Merge with umap_df to include the \"Cluster\" column\n",
        "    first_occurrence = first_occurrence.merge(filtered_subset_df[['ID_unique', 'Condition']], on='ID_unique')\n",
        "\n",
        "    first_occurrence = first_occurrence.drop_duplicates()\n",
        "\n",
        "    # Create a combined plot with boxplot and stripplot\n",
        "    boxplot = sns.boxplot(data=first_occurrence, x='Condition', y=feature, hue='Condition',\n",
        "                          showfliers=False, palette=color_map, legend=False,\n",
        "                          order=condition_order, ax=axes[i])\n",
        "    sns.stripplot(data=first_occurrence, x='Condition', y=feature, hue='Condition',\n",
        "                  jitter=0.1, alpha=0.2, palette=color_map, edgecolor='black',\n",
        "                  linewidth=1, legend=False, ax=axes[i],\n",
        "                          dodge = False)\n",
        "\n",
        "    axes[i].set_title(f'Boxplot of {feature} by Condition')\n",
        "    axes[i].set_xlabel('Condition')\n",
        "    axes[i].tick_params(axis='x', rotation=30)\n",
        "    axes[i].set_ylabel(feature)\n",
        "    axes[0].set_yticks([0,970.3,1940.7,2911,3881.3, 4851.7, 5822])\n",
        "    axes[0].set_yticklabels(['0','200', '400', '600','800','1000', '1200'])\n",
        "\n",
        "        # Step 4: Apply hatches to boxplot\n",
        "    num_conditions = len(condition_order)\n",
        "    for i, box in enumerate(boxplot.patches):\n",
        "        condition_index = i % num_conditions\n",
        "        condition = condition_order[condition_index]\n",
        "        hatch = hatch_dict[condition]\n",
        "        if hatch:\n",
        "            box.set_hatch(hatch)\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "fig.tight_layout()\n",
        "\n",
        "with mpl.rc_context({'text.usetex': False,\n",
        "                \"svg.fonttype\": 'none'\n",
        "                }):\n",
        "    plt.savefig(\"/content/gdrive/MyDrive/Condition_initial_features.svg\", format = 'svg')  # Save 2D plot as PDF\n",
        "\n",
        "# Show the figure\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeYztiB3F6uz"
      },
      "outputs": [],
      "source": [
        "#@title Perform ANOVA on intials features per condition\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd, MultiComparison\n",
        "\n",
        "# Define the feature to analyze\n",
        "feature = 'area_zero'\n",
        "\n",
        "# Extract the first occurrence for each ID and merge with condition\n",
        "first_occurrence = filtered_df.groupby('ID_unique')[feature].first().reset_index()\n",
        "first_occurrence = first_occurrence.merge(filtered_df[['ID_unique', 'Condition']], on='ID_unique')\n",
        "first_occurrence = first_occurrence.drop_duplicates()\n",
        "\n",
        "# Split data by condition\n",
        "groups = first_occurrence.groupby('Condition')[feature]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "fvalue, pvalue = stats.f_oneway(*[group for name, group in groups])\n",
        "print(f\"ANOVA p-value for {feature}: {pvalue}\")\n",
        "\n",
        "# Tukey's HSD test\n",
        "tukey_results = pairwise_tukeyhsd(first_occurrence[feature], first_occurrence['Condition'])\n",
        "print(tukey_results)\n",
        "\n",
        "# Sample sizes\n",
        "sample_sizes = first_occurrence['Condition'].value_counts()\n",
        "print(\"\\nSample Sizes:\\n\", sample_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform linear interpolation of chosen feature for each condition\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "METRIC_COLUMN_NAME = 'area_norm'\n",
        "MIN_FINAL_STRAIN = 0.7\n",
        "\n",
        "condition_df_list = []\n",
        "\n",
        "for condition_ in list(filtered_df['Condition'].unique()):\n",
        "    condition = filtered_df[filtered_df['Condition'] == condition_].copy()\n",
        "    condition = condition[['sample_id','ID_unique', 'strain', METRIC_COLUMN_NAME,]]\n",
        "\n",
        "    # condition.head()\n",
        "    print(condition_)\n",
        "    print(condition.shape)\n",
        "\n",
        "    # Convert non-numeric values to NaN\n",
        "    condition = condition.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Group by 'ID_unique' and filter\n",
        "    filtered_groups = condition.groupby('ID_unique').filter(lambda x: x['strain'].max() > MIN_FINAL_STRAIN)\n",
        "\n",
        "    def linear_interpolation_no_duplicates(group, num_points=200):\n",
        "        group = group.sort_values(by='strain')  # Sort by strain\n",
        "        group = group.drop_duplicates(subset='strain')  # Remove duplicates\n",
        "\n",
        "        f = interp1d(group['strain'], group[METRIC_COLUMN_NAME],\n",
        "                    kind='linear',\n",
        "                    bounds_error=False, fill_value=\"nan\")\n",
        "\n",
        "        new_strains = np.linspace(0, MIN_FINAL_STRAIN, num_points)\n",
        "        new_area_norms = f(new_strains)\n",
        "\n",
        "        return pd.Series(new_area_norms)\n",
        "\n",
        "    # Apply linear interpolation to each group\n",
        "    interpolated_data = filtered_groups.groupby('ID_unique').apply(linear_interpolation_no_duplicates)\n",
        "\n",
        "    # Transpose and reset index to get the desired format\n",
        "    final_df = interpolated_data\n",
        "    final_df.columns.name = None\n",
        "    final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Convert non-numeric values to NaN\n",
        "    final_df = final_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Drop rows with NaN values\n",
        "    # final_df = final_df.dropna()\n",
        "\n",
        "    print(final_df.shape)\n",
        "    # final_df.head()  # Display the first few rows of the final dataframe\n",
        "\n",
        "    condition_df_list.append(final_df)\n",
        "\n",
        "unique_sample_counts = btrack_df.groupby('Condition')['sample_id'].nunique()\n",
        "print(unique_sample_counts)"
      ],
      "metadata": {
        "id": "p0rtSYgz5g3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSu4mWVYo7DN"
      },
      "outputs": [],
      "source": [
        "#@title Plot tracks and average behavior VS strain per conditions\n",
        "\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "strain_values = np.linspace(0, MIN_FINAL_STRAIN, final_df.shape[-1])\n",
        "\n",
        "color_map = ['dimgray', 'darkgreen', 'black', 'limegreen']\n",
        "condition_order = ['myrRFP (from memGFP)', 'deActGS1', 'memGFP', \"myrRFP (from deActGS1)\"]\n",
        "\n",
        "unique_sample_counts = btrack_df.groupby('Condition')['sample_id'].nunique()\n",
        "ordered_counts = unique_sample_counts.reindex(condition_order, fill_value=0)\n",
        "n_exp = ordered_counts.values\n",
        "\n",
        "conditions_labels = condition_order\n",
        "print(conditions_labels)\n",
        "\n",
        "# Plotting each row (each ID_unique's interpolated data) as a separate line plot\n",
        "plt.figure(figsize=(4, 4))\n",
        "\n",
        "for final_df in condition_df_list:\n",
        "    for index, row in final_df.iterrows():\n",
        "        row_data =  np.array(row)\n",
        "        plt.plot(strain_values, row_data,\n",
        "                #  label=f'ID {int(row)}',\n",
        "                #  marker = 'o', linestyle = ''\n",
        "                alpha = 0.3,\n",
        "                )\n",
        "\n",
        "    plt.xlabel('Strain')\n",
        "    plt.ylabel(METRIC_COLUMN_NAME)\n",
        "    # plt.ylim(bottom = 0)\n",
        "    plt.title(f'# of Tracks  = {index})')\n",
        "    # plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "\n",
        "# Parameters for the Savitzky-Golay filter\n",
        "window_size = 11  # Must be an odd integer\n",
        "poly_order = 1    # Polynomial order, should be less than window_size\n",
        "\n",
        "hatch_patterns = ['//', '', '', '//']  # Define hatch patterns for the dim colors\n",
        "\n",
        "for idx_i, final_df in enumerate(condition_df_list):\n",
        "\n",
        "    # Calculate the rolling average\n",
        "    mean_values = final_df.mean()\n",
        "    mean_values_smooth = savgol_filter(mean_values, window_size, poly_order)\n",
        "    std_values = final_df.std(axis=0)\n",
        "    sem_values = std_values / np.sqrt(n_exp[idx_i])\n",
        "    sem_values_smooth = savgol_filter(sem_values, window_size, poly_order)\n",
        "\n",
        "    # Plot the smoothed data\n",
        "    plt.plot(strain_values, mean_values_smooth,\n",
        "             color=color_map[idx_i],\n",
        "             label=condition_order[idx_i],\n",
        "             alpha=0.95)\n",
        "\n",
        "    # Add hatched filling for dim colors using the hatch parameter\n",
        "    plt.fill_between(strain_values,\n",
        "                     mean_values_smooth - sem_values_smooth,\n",
        "                     mean_values_smooth + sem_values_smooth,\n",
        "                     color=color_map[idx_i],\n",
        "                     alpha=0.2,\n",
        "                     hatch=hatch_patterns[idx_i])\n",
        "\n",
        "plt.xlabel('Strain')\n",
        "plt.ylabel(METRIC_COLUMN_NAME)\n",
        "plt.legend(fontsize=12, loc='upper left')\n",
        "\n",
        "# with mpl.rc_context({'text.usetex': False,\n",
        "#                 \"svg.fonttype\": 'none'\n",
        "#                 }):\n",
        "#     plt.savefig(\"/content/gdrive/MyDrive/Condition_ARVSstrain.svg\", format = 'svg')  # Uncomment to save 2D plot as svg\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform ANCOVA to test for differences in average behavior between conditions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "model = ols('strain ~ C(Condition, Treatment(reference=\"myrRFP (from deActGS1)\")) + area_norm', filtered_df).fit()\n",
        "print(model.summary())\n",
        "\n",
        "ancova_results = sm.stats.anova_lm(model, typ=2)\n",
        "print(ancova_results)"
      ],
      "metadata": {
        "id": "5p6As-AiplgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot Last area/initial area per condition\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define custom color palette and condition order\n",
        "color_map = ['dimgray', 'lightgray', 'darkgreen', 'lightgreen']\n",
        "condition_order = ['memGFP', 'myrRFP (from memGFP)', 'deActGS1', \"myrRFP (from deActGS1)\"]\n",
        "palette = dict(zip(condition_order, color_map))  # Map conditions to colors\n",
        "\n",
        "# Define hatch patterns for each condition\n",
        "hatch_dict = {\n",
        "    'memGFP': '',\n",
        "    'myrRFP (from memGFP)': '//',\n",
        "    'deActGS1': '',\n",
        "    'myrRFP (from deActGS1)': '//'\n",
        "}\n",
        "\n",
        "# Step 1: Ensure only one value per 'ID_unique' and 'Condition'\n",
        "# Group by 'ID_unique' and calculate the mean for each 'Condition'\n",
        "unique_values_df = filtered_df.groupby(['ID_unique', 'Condition'], as_index=False).agg({'last_to_initial_area': 'mean'})\n",
        "\n",
        "# Step 2: Create boxplot\n",
        "plt.figure(figsize=(10, 12))\n",
        "sns_boxplot = sns.boxplot(\n",
        "    data=unique_values_df,  # Use unique values\n",
        "    x='Condition',\n",
        "    y='last_to_initial_area',\n",
        "    order=condition_order,  # Set the specific order\n",
        "    palette=palette,\n",
        "    showfliers=False\n",
        ")\n",
        "\n",
        "# Step 3: Add individual points on top of the boxplot with matching colors\n",
        "sns.stripplot(\n",
        "    data=unique_values_df,  # Use unique values\n",
        "    x='Condition',\n",
        "    y='last_to_initial_area',\n",
        "    order=condition_order,\n",
        "    palette=palette,\n",
        "    jitter=0.1,\n",
        "    alpha=0.5,  # Slightly higher transparency for better visibility\n",
        "    edgecolor='black',\n",
        "    linewidth=1,\n",
        "    size=8\n",
        ")\n",
        "\n",
        "# Step 4: Apply hatches to boxplot\n",
        "num_conditions = len(condition_order)\n",
        "for i, box in enumerate(sns_boxplot.patches):\n",
        "    condition_index = i % num_conditions\n",
        "    condition = condition_order[condition_index]\n",
        "    hatch = hatch_dict[condition]\n",
        "    if hatch:\n",
        "        box.set_hatch(hatch)\n",
        "\n",
        "# Step 5: Customize plot\n",
        "plt.ylabel('Last Area / Initial Area')\n",
        "plt.title('Last Area / Initial Area Ratio Across Conditions')\n",
        "plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "\n",
        "# with mpl.rc_context({'text.usetex': False,\n",
        "#                 \"svg.fonttype\": 'none'\n",
        "#                 }):\n",
        "#     plt.savefig(\"/content/gdrive/MyDrive/Condition_A_f_to_A_0.svg\", format = 'svg')  # Uncomment to save 2D plot as PDF\n",
        "\n",
        "# Step 6: Display plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5g5gU9JGeh1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform ANOVA on last/intial areas\n",
        "\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# Step 1: Perform One-Way ANOVA\n",
        "anova_results = stats.f_oneway(\n",
        "    *[unique_values_df[unique_values_df['Condition'] == condition]['last_to_initial_area'] for condition in condition_order]\n",
        ")\n",
        "\n",
        "print(\"One-way ANOVA result:\")\n",
        "print(f\"F-statistic: {anova_results.statistic}, p-value: {anova_results.pvalue}\")\n",
        "\n",
        "# If ANOVA is significant (p-value < 0.05), proceed with post-hoc tests\n",
        "\n",
        "# Step 2: Perform Post-hoc Tukey's HSD test\n",
        "# Fit the model\n",
        "model = ols('last_to_initial_area ~ C(Condition)', data=unique_values_df).fit()\n",
        "\n",
        "# Perform ANOVA for checking significance (using statsmodels)\n",
        "anova_table = anova_lm(model)\n",
        "print(\"\\nANOVA Table:\")\n",
        "print(anova_table)\n",
        "\n",
        "# Post-hoc Tukey's HSD test\n",
        "tukey = pairwise_tukeyhsd(endog=unique_values_df['last_to_initial_area'],\n",
        "                          groups=unique_values_df['Condition'],\n",
        "                          alpha=0.05)\n",
        "\n",
        "# Print Tukey's HSD test result\n",
        "print(\"\\nPost-hoc Tukey HSD Test Results:\")\n",
        "print(tukey.summary())\n",
        "\n",
        "# # Print the number of samples (n) for each condition\n",
        "# n_per_condition = merged_df.groupby('Condition')['last_to_initial_area'].count()\n",
        "# print(\"\\nNumber of samples (n) per condition:\")\n",
        "# print(n_per_condition)"
      ],
      "metadata": {
        "id": "MnobuOqAMwjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}